name: Run SLURM Jobs

permissions:
  contents: write

on:
  push:
    paths:
      - "input/**"

jobs:
  run-slurm:
    runs-on: self-hosted
    timeout-minutes: 7200

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 1
          ref: main
          fetch-tags: false

      - name: Prepare working and output directories
        run: |
          set -euo pipefail
          mkdir -p output
          export SLURM_TMP_BASE="/tmp/slurm_jobs_${GITHUB_RUN_ID}"
          mkdir -p "$SLURM_TMP_BASE"
          echo "SLURM_TMP_BASE=$SLURM_TMP_BASE" >> "$GITHUB_ENV"
          # Initialize failure flag
          echo "JOBSET_FAILED=0" >> "$GITHUB_ENV"

      - name: Copy nested job folders to SLURM temp dir
        run: |
          set -euo pipefail
          shopt -s nullglob

          copied_any=0
          for jobdir in input/{HPL-MxP_NVIDIA,HPL_NVIDIA,HPL,SST,ECE}/{Ayush,Beno,Daniel,John,Nicholas,Sithum,Will}/*; do
            [ -d "$jobdir" ] || continue
            relpath="${jobdir#input/}"
            dest="${SLURM_TMP_BASE}/${relpath}"
            mkdir -p "$dest"
            # Copy including hidden files
            cp -r "$jobdir"/. "$dest"/ || true
            echo "Copied $jobdir -> $dest"
            copied_any=1
          done

          if [ "$copied_any" -eq 0 ]; then
            echo "No job directories found under input/{HPL-MxP_NVIDIA,HPL_NVIDIA,HPL,SST,ECE}/{Ayush,Beno,Daniel,John,Nicholas,Sithum,Will}/*"
          fi

          echo "Directory structure in temp dir:"
          command -v tree >/dev/null && tree "$SLURM_TMP_BASE" || ls -R "$SLURM_TMP_BASE"

      - name: Discover SLURM nodes
        run: |
          set -euo pipefail
          # Collect nodes in useful states (exclude down/drain)
          sinfo -N -h -t idle,alloc,mix,comp -o "%n" | sort -u > nodes.txt
          node_count="$(wc -l < nodes.txt | tr -d ' ')"
          echo "Found $node_count nodes:"
          cat nodes.txt || true
          echo "NODES_FILE=$(pwd)/nodes.txt" >> "$GITHUB_ENV"

      - name: Ensure binaries exist in each job folder (symlink if missing)
        run: |
          set -euo pipefail

          for jobdir in "${SLURM_TMP_BASE}"/{HPL-MxP_NVIDIA,HPL_NVIDIA,HPL,SST,ECE}/{Ayush,Beno,Daniel,John,Nicholas,Sithum,Will}/*; do
            [ -d "$jobdir" ] || continue

            case "$jobdir" in
              *HPL_NVIDIA*) src="/_HPC/defaults/HPL_NVIDIA/xhpl-nvidia" ; bin_name="xhpl-nvidia" ;;
              *HPL-MxP_NVIDIA*) src="/_HPC/defaults/HPL-MxP_NVIDIA/xhpl_mxp-nvidia" ; bin_name="xhpl_mxp-nvidia" ;;
              *HPL*)        src="/_HPC/defaults/hpl-2.3/testing/xhpl"   ; bin_name="xhpl" ;;
              *SST*)        src="/opt/sst/sst_binary"              ; bin_name="sst_binary" ;;
              *ECE*)        src="/opt/ece/ece_binary"              ; bin_name="ece_binary" ;;
              *)            src="" ; bin_name="" ;;
            esac

            [ -n "$src" ] || continue

            if [ ! -x "$jobdir/$bin_name" ]; then
              echo "No $bin_name in $jobdir, symlinking from $src"
              ln -sf "$src" "$jobdir/$bin_name"
            fi
          done


      - name: Distribute job folders to all cluster nodes
        run: |
          set -euo pipefail
          nodes_file="${NODES_FILE:-nodes.txt}"
          remote_base="${SLURM_TMP_BASE}"
          : "${DISTRIBUTION_CONCURRENCY:=8}"

          if [ ! -s "$nodes_file" ]; then
            echo "No nodes found; skipping distribution."
            exit 0
          fi

          echo "Distributing $SLURM_TMP_BASE to all nodes..."
          failures=0
          running=0

          copy_to_node() {
            local node="$1"
            echo "[${node}] creating $remote_base"
            if ! ssh -o BatchMode=yes -o StrictHostKeyChecking=no \
              "$node" "mkdir -p '$remote_base'"
            then
              echo "[${node}] mkdir failed"
              return 1
            fi

            if ssh -o BatchMode=yes -o StrictHostKeyChecking=no \
              "$node" 'command -v rsync >/dev/null 2>&1'
            then
              echo "[${node}] rsync -> $remote_base"
              rsync -a --delete \
                -e "ssh -o BatchMode=yes -o StrictHostKeyChecking=no" \
                "$SLURM_TMP_BASE"/ "$node:$remote_base"/
            else
              echo "[${node}] tar stream -> $remote_base"
              tar -C "$SLURM_TMP_BASE" -cf - . \
                | ssh -o BatchMode=yes -o StrictHostKeyChecking=no \
                  "$node" "tar -C '$remote_base' -xf -"
            fi
          }

          while IFS= read -r node; do
            [ -n "$node" ] || continue
            (
              if ! copy_to_node "$node"; then
                echo "[${node}] distribution failed"
                exit 1
              fi
            ) &
            running=$((running + 1))
            if [ "$running" -ge "$DISTRIBUTION_CONCURRENCY" ]; then
              if ! wait -n; then
                failures=$((failures + 1))
              fi
              running=$((running - 1))
            fi
          done < "$nodes_file"

          # Wait for remaining background jobs
          while [ "$running" -gt 0 ]; do
            if ! wait -n; then
              failures=$((failures + 1))
            fi
            running=$((running - 1))
          done

          if [ "$failures" -ne 0 ]; then
            echo "Distribution encountered $failures failures."
            exit 1
          fi
          echo "Distribution completed successfully."

      - name: Submit jobs to SLURM (.sh files in each job folder)
        run: |
          set -euo pipefail
          shopt -s nullglob

          : > joblist.txt
          any_submitted=0

          for jobdir in "${SLURM_TMP_BASE}"/{HPL-MxP_NVIDIA,HPL_NVIDIA,HPL,SST,ECE}/{Ayush,Beno,Daniel,John,Nicholas,Sithum,Will}/*; do
            [ -d "$jobdir" ] || continue
            jobrel="${jobdir#${SLURM_TMP_BASE}/}"

            scripts_found=0
            for script in "$jobdir"/*.sh; do
              [ -f "$script" ] || continue
              scripts_found=1
              fname="$(basename "$script")"
              echo "Submitting $fname from $jobrel..."
              jobid="$(sbatch --parsable \
                --chdir="$jobdir" \
                --output="${jobdir}/${fname}-%j.out" \
                --error="${jobdir}/${fname}-%j.err" \
                "$script")"
              echo "${jobrel}|${fname}:$jobid" >> joblist.txt
              any_submitted=1
            done

            if [ "$scripts_found" -eq 0 ]; then
              echo "No .sh scripts found in $jobrel; skipping." >&2
            fi
          done

          if [ "$any_submitted" -eq 0 ]; then
            echo "No jobs submitted. Nothing to do."
          else
            echo "Submitted jobs:"
            cat joblist.txt || true
          fi

      - name: Wait for jobs to finish (with live completion logs)
        run: |
          set -euo pipefail
        
          if [ ! -s joblist.txt ]; then
            echo "No jobs to wait for."
            exit 0
          fi
        
          # Build mapping file: jobid<TAB>jobrel<TAB>script
          awk -F'[:|]' '{print $3 "\t" $1 "\t" $2}' joblist.txt > jobmap.tsv
          # Initialize remaining job IDs to track
          awk -F: '{print $2}' joblist.txt | sort -u > remaining.txt
        
          total="$(wc -l < remaining.txt | tr -d ' ')"
          completed=0
          : "${POLL_INTERVAL:=10}"
        
          echo "Waiting for $total SLURM jobs to complete (poll ${POLL_INTERVAL}s)..."
        
          lookup() {
            # Echoes: jobrel<TAB>script for given jobid
            local id="$1"
            awk -v id="$id" 'BEGIN{FS="\t"} $1==id {print $2"\t"$3; exit}' \
              jobmap.tsv
          }
        
          final_state() {
            # Echoes: state exitcode for given jobid
            local id="$1"
            if command -v sacct >/dev/null 2>&1; then
              local line
              line="$(sacct -n -P -j "$id" -o State,ExitCode 2>/dev/null \
                | tail -n1 || true)"
              if [ -n "${line:-}" ]; then
                echo "${line%%|*} ${line##*|}"
                return
              fi
            fi
            local info state exitcode
            info="$(scontrol show job "$id" 2>/dev/null || true)"
            state="$(awk -F= '/JobState=/{print $2}' <<< "$info" \
              | awk '{print $1}')"
            exitcode="$(awk -F= '/ExitCode=/{print $2}' <<< "$info" \
              | awk '{print $1}')"
            echo "${state:-UNKNOWN} ${exitcode:-N/A}"
          }
        
          touch active_base.txt remaining.sorted just_finished.txt
        
          while [ -s remaining.txt ]; do
            # Query active jobs among the remaining list in one call.
            # For array jobs, squeue shows IDs like 1234_5, so strip the
            # suffix to match the parent job ID.
            squeue -h -o "%i" -j "$(paste -sd, - < remaining.txt)" \
              2>/dev/null \
              | sed 's/_.*$//' \
              | sort -u > active_base.txt || true
        
            sort -u remaining.txt > remaining.sorted
        
            if [ -s active_base.txt ]; then
              # Jobs that just left the queue: remaining - active
              comm -23 remaining.sorted active_base.txt > just_finished.txt
            else
              # None active => everything remaining just finished
              cp remaining.sorted just_finished.txt
            fi
        
            if [ -s just_finished.txt ]; then
              while read -r jobid; do
                [ -n "$jobid" ] || continue
                IFS=$'\t' read -r jobrel script < <(lookup "$jobid")
                read -r state exitcode < <(final_state "$jobid")
                completed=$((completed + 1))
                echo "Job $jobid ($script in $jobrel) finished: state=$state exitcode=$exitcode [$completed/$total]"
              done < just_finished.txt
            else
              left="$(wc -l < remaining.txt | tr -d ' ')"
              echo "Still running: $left/$total jobs remaining..."
            fi
        
            # Next loop will only track the ones still active
            if [ -s active_base.txt ]; then
              mv active_base.txt remaining.txt
            else
              : > remaining.txt
            fi
        
            [ -s remaining.txt ] && sleep "$POLL_INTERVAL" || true
          done
        
          echo "All $total jobs have left the queue."

      - name: Check job exit codes and mark completed job dirs
        run: |
          set -euo pipefail

          if [ ! -s joblist.txt ]; then
            echo "No jobs submitted; skipping checks."
            : > completed_dirs.txt
            : > jobdirs_all_sorted.txt
            exit 0
          fi

          echo "Checking SLURM job exit codes..."
          failed_any=0

          # Build list of all job directories involved (relative to input/)
          cut -d: -f1 joblist.txt | awk -F'|' '{print $1}' | sort -u \
            > jobdirs_all_sorted.txt

          : > failed_dirs.txt

          while read -r line; do
            [ -n "$line" ] || continue
            entry="${line%%:*}"   # jobrel|script
            jobid="${line##*:}"
            jobrel="${entry%%|*}"
            script="${entry##*|}"

            # Robustly fetch job info without failing the step if missing
            jobinfo="$(scontrol show job "$jobid" 2>/dev/null || true)"
            state="$(awk -F= '/JobState=/{print $2}' <<< "$jobinfo" | awk '{print $1}')"
            exitcode="$(awk -F= '/ExitCode=/{print $2}' <<< "$jobinfo" | awk '{print $1}')"
            if [ -z "${state:-}" ]; then
              state="UNKNOWN"
              exitcode="N/A"
            fi

            echo "Job $jobid ($script) in $jobrel finished with state=$state exitcode=$exitcode"
            if [[ "$state" != "COMPLETED" ]]; then
              failed_any=1
              echo "$jobrel" >> failed_dirs.txt
            fi
          done < joblist.txt

          sort -u failed_dirs.txt > failed_dirs_sorted.txt || true

          # Job directories where all submitted scripts completed successfully
          comm -23 jobdirs_all_sorted.txt failed_dirs_sorted.txt \
            > completed_dirs.txt || true

          echo "Job directories with all jobs completed successfully:"
          cat completed_dirs.txt || echo "(none)"

          if [[ $failed_any -ne 0 ]]; then
            echo "One or more jobs failed at the Slurm level."
            echo "JOBSET_FAILED=1" >> "$GITHUB_ENV"
          else
            echo "JOBSET_FAILED=0" >> "$GITHUB_ENV"
          fi

      - name: Gather output files from all nodes
        if: always()
        run: |
          set -euo pipefail
          shopt -s nullglob
          nodes_file="${NODES_FILE:-nodes.txt}"

          mkdir -p output

          if [ ! -s "$nodes_file" ]; then
            echo "No nodes found; skipping output collection."
            exit 0
          fi

          echo "Collecting outputs from all nodes..."
          failures=0
          running=0
          : "${GATHER_CONCURRENCY:=8}"

          fetch_from_node() {
            local node="$1"
            # Skip node if remote dir doesn't exist
            if ! ssh -o BatchMode=yes -o StrictHostKeyChecking=no \
              "$node" "test -d '$SLURM_TMP_BASE'"
            then
              echo "[${node}] $SLURM_TMP_BASE not found; skipping."
              return 0
            fi

            if ssh -o BatchMode=yes -o StrictHostKeyChecking=no \
              "$node" 'command -v rsync >/dev/null 2>&1'
            then
              echo "[${node}] rsync -> output/"
              rsync -a --ignore-errors \
                -e "ssh -o BatchMode=yes -o StrictHostKeyChecking=no" \
                "$node:$SLURM_TMP_BASE"/ output/
            else
              echo "[${node}] tar stream -> output/"
              ssh -o BatchMode=yes -o StrictHostKeyChecking=no \
                "$node" "tar -C '$SLURM_TMP_BASE' -cf - ." \
                | tar -C output -xf -
            fi
          }

          while IFS= read -r node; do
            [ -n "$node" ] || continue
            (
              if ! fetch_from_node "$node"; then
                echo "[${node}] fetch failed"
                exit 1
              fi
            ) &
            running=$((running + 1))
            if [ "$running" -ge "$GATHER_CONCURRENCY" ]; then
              if ! wait -n; then
                failures=$((failures + 1))
              fi
              running=$((running - 1))
            fi
          done < "$nodes_file"

          while [ "$running" -gt 0 ]; do
            if ! wait -n; then
              failures=$((failures + 1))
            fi
            running=$((running - 1))
          done

          if [ "$failures" -ne 0 ]; then
            echo "Output collection encountered $failures failures."
            exit 1
          fi

          # Clean up local staging after collection
          rm -rf "${SLURM_TMP_BASE}"
 
      - name: Prune symlinked binaries from output
        if: always()
        run: |
          set -euo pipefail
          if [ -d output ]; then
            echo "Removing symlinked job binaries from output/ ..."
            # Only remove the known binary names we symlinked for runs.
            find output -type l \
              \( -name 'xhpl' -o -name 'xhpl-nvidia' -o -name 'xhpl_mxp-nvidia' \
                 -o -name 'sst_binary' -o -name 'ece_binary' \) \
              -print -delete || true
          fi
          
      - name: Cleanup remote staging on all nodes
        if: always()
        run: |
          set -euo pipefail
          nodes_file="${NODES_FILE:-nodes.txt}"
          if [ ! -s "$nodes_file" ]; then
            echo "No nodes found; skipping remote cleanup."
            exit 0
          fi

          echo "Removing $SLURM_TMP_BASE from all nodes..."
          pdsh_ok=0
          if command -v pdsh >/dev/null 2>&1; then
            pdsh_ok=1
          fi

          if [ "$pdsh_ok" -eq 1 ]; then
            pdsh -R ssh -w ^"$nodes_file" "rm -rf '$SLURM_TMP_BASE'" || true
          else
            while IFS= read -r node; do
              [ -n "$node" ] || continue
              ssh -o BatchMode=yes -o StrictHostKeyChecking=no \
                "$node" "rm -rf '$SLURM_TMP_BASE'" || true
            done < "$nodes_file"
          fi

      - name: Commit output and remove input job dirs (success or fail)
        if: always()
        run: |
          set -euo pipefail

          # Remove input job directories for all submitted jobs (regardless of success)
          if [ -s joblist.txt ]; then
            echo "Removing input directories for all submitted jobs..."
            cut -d: -f1 joblist.txt | awk -F'|' '{print $1}' | sort -u \
              > jobdirs_to_remove.txt
            while read -r d; do
              [ -n "$d" ] || continue
              if [ -d "input/$d" ]; then
                rm -rf "input/$d"
                echo "Deleted input/$d"
              fi
            done < jobdirs_to_remove.txt
          else
            echo "No recorded job submissions; skipping input cleanup."
          fi

          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          git remote set-url origin \
            "https://x-access-token:${GITHUB_TOKEN}@github.com/${GITHUB_REPOSITORY}.git"

          BRANCH="${GITHUB_REF_NAME:-$(git rev-parse --abbrev-ref HEAD)}"
          echo "Target branch: $BRANCH"

          # Ensure full history and that we're on the branch (not detached HEAD)
          git fetch --prune origin "$BRANCH"
          git checkout -B "$BRANCH"

          # Pull and merge any upstream changes before committing/pushing
          git pull --no-rebase --no-edit origin "$BRANCH"

          # Stage outputs and input deletions
          git add -A output/ input/
          if git diff --cached --quiet; then
            echo "Nothing new to commit after staging."
          else
            git commit -m "Add SLURM job output and purge inputs (incl. failed jobs) from run ${GITHUB_RUN_ID} [skip ci]"
          fi

          # Push; if remote changed meanwhile, pull+merge and retry (up to 3 tries)
          for attempt in 1 2 3; do
            if git push origin "$BRANCH"; then
              echo "Push succeeded."
              break
            fi
            echo "Push failed (attempt $attempt). Remote changed; pulling and merging, then retrying..."
            git pull --no-rebase --no-edit origin "$BRANCH"
            if [ "$attempt" -eq 3 ]; then
              echo "Giving up after $attempt attempts."
              exit 1
            fi
          done
        env:
          GITHUB_TOKEN: ${{ secrets.PAT }}

      - name: Upload output as artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: slurm-job-output
          path: output/