
================================================================================
HPL-NVIDIA 25.4.0  -- NVIDIA accelerated HPL benchmark -- NVIDIA
================================================================================
HPLinpack 2.1  --  High-Performance Linpack benchmark  --   October 26, 2012
Written by A. Petitet and R. Clint Whaley,  Innovative Computing Laboratory, UTK
Modified by Piotr Luszczek, Innovative Computing Laboratory, UTK
Modified by Julien Langou, University of Colorado Denver
================================================================================

An explanation of the input/output parameters follows:
T/V    : Wall time / encoded variant.
N      : The order of the coefficient matrix A.
NB     : The partitioning blocking factor.
P      : The number of process rows.
Q      : The number of process columns.
Time   : Time in seconds to solve the linear system.
Gflops : Rate of execution for solving the linear system.

The following parameter values will be used:

N      :   50000 
NB     :    1024 
PMAP   : Column-major process mapping
P      :       2 
Q      :       2 
PFACT  :    Left 
NBMIN  :       2 
NDIV   :       2 
RFACT  :    Left 
BCAST  :  2ringM 
DEPTH  :       1 
SWAP   : Spread-roll (long)
L1     : no-transposed form
U      : transposed form
EQUIL  : no
ALIGN  : 8 double precision words

--------------------------------------------------------------------------------

- The matrix A is randomly generated for each test.
- The following scaled residual check will be computed:
      ||Ax-b||_oo / ( eps * ( || x ||_oo * || A ||_oo + || b ||_oo ) * N )
- The relative machine precision (eps) is taken to be               1.110223e-16
- Computational tests pass if scaled residuals are less than                16.0


HPL-NVIDIA ignores the following parameters from input file:
	* Broadcast parameters
	* Panel factorization parameters
	* Look-ahead value
	* L1 layout
	* U layout
	* Equilibration parameter
	* Memory alignment parameter

HPL-NVIDIA settings from environment variables:
--- DEVICE INFO ---
  Peak clock frequency: 1410 MHz
  SM version          : 80
  Number of SMs       : 108
-------------------
[HPL TRACE] cuda_nvshmem_init: max=8.8423 (0) min=8.8412 (1)
[WARNING] Change Input N 50000 to 49152
[HPL TRACE] ncclCommInitRank: max=0.8634 (1) min=0.8425 (2)
[HPL TRACE] cugetrfs_mp_init: max=0.9213 (2) min=0.9212 (1)
--- MEMORY INFO ---
DEVICE
  System           =      6.22768 GiB (MIN)      6.57925 GiB (MAX)      6.40347 GiB (AVG)
  HPL buffers      =      8.20530 GiB (MIN)      8.20530 GiB (MAX)      8.20530 GiB (AVG)
  Used             =     14.43298 GiB (MIN)     14.78455 GiB (MAX)     14.60876 GiB (AVG)
  Total            =     40.00000 GiB (MIN)     40.00000 GiB (MAX)     40.00000 GiB (AVG)
HOST
  HPL buffers      =      0.00023 GiB (MIN)      0.00023 GiB (MAX)      0.00023 GiB (AVG)
-------------------

 ... Testing HPL components ... 

 **** Factorization, m = 24576, policy = 0 **** 
avg time =    28.50 ms, avg =   904.15. min =   868.44 [rank 0, host node1, gpuID 0000:01:00.0], max =   942.91 GFLOPS

 **** Factorization, m = 24576, policy = 1 **** 
avg time =    39.90 ms, avg =   645.92. min =   516.93 [rank 2, host node1, gpuID 0000:81:00.0], max =   860.74 GFLOPS

 **** Factorization, m = 12288, policy = 0 **** 
avg time =    21.11 ms, avg =   610.38. min =   609.11 [rank 2, host node1, gpuID 0000:81:00.0], max =   611.66 GFLOPS

 **** Factorization, m = 12288, policy = 1 **** 
avg time =    36.44 ms, avg =   353.63. min =   278.31 [rank 2, host node1, gpuID 0000:81:00.0], max =   484.83 GFLOPS

 **** Factorization, m = 6144, policy = 0 **** 
avg time =    19.19 ms, avg =   335.64. min =   335.02 [rank 2, host node1, gpuID 0000:81:00.0], max =   336.26 GFLOPS

 **** Factorization, m = 6144, policy = 1 **** 
avg time =    34.44 ms, avg =   187.04. min =   146.19 [rank 2, host node1, gpuID 0000:81:00.0], max =   259.59 GFLOPS

 **** Factorization, m = 1024, policy = 0 **** 
avg time =    17.25 ms, avg =    62.25. min =    62.17 [rank 2, host node1, gpuID 0000:81:00.0], max =    62.34 GFLOPS

 **** Factorization, m = 1024, policy = 1 **** 
avg time =    32.87 ms, avg =    32.66. min =    25.15 [rank 2, host node1, gpuID 0000:81:00.0], max =    46.58 GFLOPS

 **** ncclBcast( Row ) **** 
avg time =     0.00 ms, avg = 321146.26. min = 298305.81 [rank 1, host node1, gpuID 0000:41:00.0], max = 350804.31 GBS

 **** ncclAllGather( Col ) **** 
avg time =     0.00 ms, avg = 256385.34. min = 248551.35 [rank 0, host node1, gpuID 0000:01:00.0], max = 261633.00 GBS

 **** Latency ncclAllGather, m = 1 **** 
avg time =     0.29 ms, avg =     0.06. min =     0.06 [rank 1, host node1, gpuID 0000:41:00.0], max =     0.06 GBS

 **** Latency ncclAllGather, m = 2 **** 
avg time =     0.29 ms, avg =     0.11. min =     0.11 [rank 1, host node1, gpuID 0000:41:00.0], max =     0.11 GBS

 **** Latency ncclAllGather, m = 32 **** 
avg time =     0.29 ms, avg =     1.79. min =     1.78 [rank 1, host node1, gpuID 0000:41:00.0], max =     1.80 GBS

 **** Latency ncclAllGather, m = 1024 **** 
avg time =     0.29 ms, avg =    57.22. min =    57.11 [rank 2, host node1, gpuID 0000:81:00.0], max =    57.34 GBS

 **** Latency ncclAllGather, m = 2048 **** 
avg time =     0.29 ms, avg =   114.55. min =   114.38 [rank 2, host node1, gpuID 0000:81:00.0], max =   114.72 GBS

 **** Latency Host MPI_Allgather, m = 1 **** 
avg time =     0.73 ms, avg =     0.02. min =     0.02 [rank 3, host node1, gpuID 0000:C1:00.0], max =     0.02 GBS

 **** Latency Host MPI_Allgather, m = 2 **** 
avg time =     0.73 ms, avg =     0.04. min =     0.04 [rank 1, host node1, gpuID 0000:41:00.0], max =     0.04 GBS

 **** Latency Host MPI_Allgather, m = 32 **** 
avg time =     0.85 ms, avg =     0.62. min =     0.62 [rank 2, host node1, gpuID 0000:81:00.0], max =     0.62 GBS

 **** Latency Host MPI_Allgather, m = 1024 **** 
avg time =     6.34 ms, avg =     2.65. min =     2.64 [rank 1, host node1, gpuID 0000:41:00.0], max =     2.66 GBS

 **** Latency Host MPI_Allgather, m = 2048 **** 
avg time =    11.47 ms, avg =     2.93. min =     2.90 [rank 2, host node1, gpuID 0000:81:00.0], max =     2.95 GBS

 **** Latency ncclBcast, m = 1 **** 
avg time =     0.31 ms, avg =     0.03. min =     0.03 [rank 0, host node1, gpuID 0000:01:00.0], max =     0.03 GBS

 **** Latency ncclBcast, m = 32 **** 
avg time =     0.31 ms, avg =     0.83. min =     0.83 [rank 2, host node1, gpuID 0000:81:00.0], max =     0.84 GBS

 **** Latency ncclBcast, m = 1024 **** 
avg time =     0.31 ms, avg =    26.74. min =    26.63 [rank 2, host node1, gpuID 0000:81:00.0], max =    26.87 GBS

 **** Latency Host MPI_Bcast, m = 1 **** 
avg time =     0.12 ms, avg =     0.07. min =     0.07 [rank 3, host node1, gpuID 0000:C1:00.0], max =     0.07 GBS

 **** Latency Host MPI_Bcast, m = 32 **** 
avg time =     0.16 ms, avg =     1.62. min =     1.59 [rank 3, host node1, gpuID 0000:C1:00.0], max =     1.65 GBS

 **** Latency Host MPI_Bcast, m = 1024 **** 
avg time =     0.58 ms, avg =    14.48. min =    12.70 [rank 1, host node1, gpuID 0000:41:00.0], max =    16.85 GBS

 **** GEMM **** 
avg time =    44.65 ms, avg = 18468.11. min = 18269.20 [rank 2, host node1, gpuID 0000:81:00.0], max = 18629.34 GFLOPS

 ... End of Testing HPL components ... 

[HPL TRACE] HPL_pdmatgen_gpu: max=0.0038 (3) min=0.0036 (1)
