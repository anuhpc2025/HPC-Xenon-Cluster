Rank 0 on host node1, GPU: 
Rank 0 on host node1, GPU: 
Rank 0 on host node1, GPU: 
Rank 0 on host node1, GPU: 

================================================================================
HPL-NVIDIA 25.4.0  -- NVIDIA accelerated HPL benchmark -- NVIDIA
================================================================================
HPLinpack 2.1  --  High-Performance Linpack benchmark  --   October 26, 2012
Written by A. Petitet and R. Clint Whaley,  Innovative Computing Laboratory, UTK
Modified by Piotr Luszczek, Innovative Computing Laboratory, UTK
Modified by Julien Langou, University of Colorado Denver
================================================================================

An explanation of the input/output parameters follows:
T/V    : Wall time / encoded variant.
N      : The order of the coefficient matrix A.
NB     : The partitioning blocking factor.
P      : The number of process rows.
Q      : The number of process columns.
Time   : Time in seconds to solve the linear system.
Gflops : Rate of execution for solving the linear system.

The following parameter values will be used:

N      :   25000 
NB     :    1024 
PMAP   : Column-major process mapping
P      :       2 
Q      :       2 
PFACT  :    Left 
NBMIN  :       2 
NDIV   :       2 
RFACT  :    Left 
BCAST  :  2ringM 
DEPTH  :       1 
SWAP   : Spread-roll (long)
L1     : no-transposed form
U      : transposed form
EQUIL  : no
ALIGN  : 8 double precision words

--------------------------------------------------------------------------------

- The matrix A is randomly generated for each test.
- The following scaled residual check will be computed:
      ||Ax-b||_oo / ( eps * ( || x ||_oo * || A ||_oo + || b ||_oo ) * N )
- The relative machine precision (eps) is taken to be               1.110223e-16
- Computational tests pass if scaled residuals are less than                16.0


HPL-NVIDIA ignores the following parameters from input file:
	* Broadcast parameters
	* Panel factorization parameters
	* Look-ahead value
	* L1 layout
	* U layout
	* Equilibration parameter
	* Memory alignment parameter

HPL-NVIDIA settings from environment variables:
--- DEVICE INFO ---
  Peak clock frequency: 1410 MHz
  SM version          : 80
  Number of SMs       : 108
-------------------
[HPL TRACE] cuda_nvshmem_init: max=8.8439 (2) min=8.8439 (3)
[WARNING] Change Input N 25000 to 24576
[HPL TRACE] ncclCommInitRank: max=0.8802 (3) min=0.8791 (2)
[HPL TRACE] cugetrfs_mp_init: max=0.9287 (0) min=0.9287 (3)
--- MEMORY INFO ---
DEVICE
  System           =      6.00921 GiB (MIN)      6.36077 GiB (MAX)      6.18499 GiB (AVG)
  HPL buffers      =      4.54804 GiB (MIN)      4.54804 GiB (MAX)      4.54804 GiB (AVG)
  Used             =     10.55725 GiB (MIN)     10.90881 GiB (MAX)     10.73303 GiB (AVG)
  Total            =     40.00000 GiB (MIN)     40.00000 GiB (MAX)     40.00000 GiB (AVG)
HOST
  HPL buffers      =      0.00014 GiB (MIN)      0.00014 GiB (MAX)      0.00014 GiB (AVG)
-------------------

 ... Testing HPL components ... 

 **** Factorization, m = 12288, policy = 0 **** 
avg time =    25.66 ms, avg =   502.19. min =   492.09 [rank 2, host node1, gpuID 0000:81:00.0], max =   512.71 GFLOPS

 **** Factorization, m = 12288, policy = 1 **** 
avg time =    35.59 ms, avg =   362.00. min =   293.29 [rank 2, host node1, gpuID 0000:81:00.0], max =   472.77 GFLOPS

 **** Factorization, m = 6144, policy = 0 **** 
avg time =    19.33 ms, avg =   333.28. min =   332.12 [rank 2, host node1, gpuID 0000:81:00.0], max =   334.46 GFLOPS

 **** Factorization, m = 6144, policy = 1 **** 
avg time =    33.36 ms, avg =   193.13. min =   154.37 [rank 2, host node1, gpuID 0000:81:00.0], max =   257.91 GFLOPS

 **** Factorization, m = 3072, policy = 0 **** 
avg time =    18.03 ms, avg =   178.68. min =   178.55 [rank 2, host node1, gpuID 0000:81:00.0], max =   178.80 GFLOPS

 **** Factorization, m = 3072, policy = 1 **** 
avg time =    32.37 ms, avg =    99.51. min =    78.96 [rank 2, host node1, gpuID 0000:81:00.0], max =   134.52 GFLOPS

 **** Factorization, m = 1024, policy = 0 **** 
avg time =    17.20 ms, avg =    62.41. min =    62.05 [rank 2, host node1, gpuID 0000:81:00.0], max =    62.77 GFLOPS

 **** Factorization, m = 1024, policy = 1 **** 
avg time =    31.70 ms, avg =    33.88. min =    26.64 [rank 2, host node1, gpuID 0000:81:00.0], max =    46.52 GFLOPS

 **** ncclBcast( Row ) **** 
avg time =     0.00 ms, avg = 130224.19. min = 90524.55 [rank 2, host node1, gpuID 0000:81:00.0], max = 163414.44 GBS

 **** ncclAllGather( Col ) **** 
avg time =     0.00 ms, avg = 189483.85. min = 184027.96 [rank 0, host node1, gpuID 0000:01:00.0], max = 198939.32 GBS

 **** Latency ncclAllGather, m = 1 **** 
avg time =     0.29 ms, avg =     0.06. min =     0.06 [rank 3, host node1, gpuID 0000:C1:00.0], max =     0.06 GBS

 **** Latency ncclAllGather, m = 2 **** 
avg time =     0.29 ms, avg =     0.11. min =     0.11 [rank 1, host node1, gpuID 0000:41:00.0], max =     0.11 GBS

 **** Latency ncclAllGather, m = 32 **** 
avg time =     0.29 ms, avg =     1.79. min =     1.79 [rank 3, host node1, gpuID 0000:C1:00.0], max =     1.79 GBS

 **** Latency ncclAllGather, m = 1024 **** 
avg time =     0.29 ms, avg =    57.29. min =    57.10 [rank 3, host node1, gpuID 0000:C1:00.0], max =    57.41 GBS

 **** Latency ncclAllGather, m = 2048 **** 
avg time =     0.29 ms, avg =   114.58. min =   114.28 [rank 3, host node1, gpuID 0000:C1:00.0], max =   114.72 GBS

 **** Latency Host MPI_Allgather, m = 1 **** 
avg time =     0.73 ms, avg =     0.02. min =     0.02 [rank 2, host node1, gpuID 0000:81:00.0], max =     0.02 GBS

 **** Latency Host MPI_Allgather, m = 2 **** 
avg time =     0.73 ms, avg =     0.04. min =     0.04 [rank 2, host node1, gpuID 0000:81:00.0], max =     0.05 GBS

 **** Latency Host MPI_Allgather, m = 32 **** 
avg time =     0.84 ms, avg =     0.62. min =     0.62 [rank 2, host node1, gpuID 0000:81:00.0], max =     0.63 GBS

 **** Latency Host MPI_Allgather, m = 1024 **** 
avg time =     6.36 ms, avg =     2.64. min =     2.63 [rank 3, host node1, gpuID 0000:C1:00.0], max =     2.64 GBS

 **** Latency Host MPI_Allgather, m = 2048 **** 
avg time =    11.66 ms, avg =     2.88. min =     2.85 [rank 0, host node1, gpuID 0000:01:00.0], max =     2.90 GBS

 **** Latency ncclBcast, m = 1 **** 
avg time =     0.31 ms, avg =     0.03. min =     0.03 [rank 0, host node1, gpuID 0000:01:00.0], max =     0.03 GBS

 **** Latency ncclBcast, m = 32 **** 
avg time =     0.31 ms, avg =     0.84. min =     0.84 [rank 0, host node1, gpuID 0000:01:00.0], max =     0.84 GBS

 **** Latency ncclBcast, m = 1024 **** 
avg time =     0.31 ms, avg =    26.79. min =    26.75 [rank 3, host node1, gpuID 0000:C1:00.0], max =    26.84 GBS

 **** Latency Host MPI_Bcast, m = 1 **** 
avg time =     0.12 ms, avg =     0.07. min =     0.07 [rank 3, host node1, gpuID 0000:C1:00.0], max =     0.07 GBS

 **** Latency Host MPI_Bcast, m = 32 **** 
avg time =     0.16 ms, avg =     1.59. min =     1.55 [rank 3, host node1, gpuID 0000:C1:00.0], max =     1.65 GBS

 **** Latency Host MPI_Bcast, m = 1024 **** 
avg time =     0.53 ms, avg =    15.97. min =    15.61 [rank 3, host node1, gpuID 0000:C1:00.0], max =    16.35 GBS

 **** GEMM **** 
avg time =    16.72 ms, avg = 18497.85. min = 18272.11 [rank 2, host node1, gpuID 0000:81:00.0], max = 18650.74 GFLOPS

 ... End of Testing HPL components ... 

[HPL TRACE] HPL_pdmatgen_gpu: max=0.0011 (3) min=0.0010 (0)
