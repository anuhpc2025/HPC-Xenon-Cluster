
================================================================================
HPL-NVIDIA 25.4.0  -- NVIDIA accelerated HPL benchmark -- NVIDIA
================================================================================
HPLinpack 2.1  --  High-Performance Linpack benchmark  --   October 26, 2012
Written by A. Petitet and R. Clint Whaley,  Innovative Computing Laboratory, UTK
Modified by Piotr Luszczek, Innovative Computing Laboratory, UTK
Modified by Julien Langou, University of Colorado Denver
================================================================================

An explanation of the input/output parameters follows:
T/V    : Wall time / encoded variant.
N      : The order of the coefficient matrix A.
NB     : The partitioning blocking factor.
P      : The number of process rows.
Q      : The number of process columns.
Time   : Time in seconds to solve the linear system.
Gflops : Rate of execution for solving the linear system.

The following parameter values will be used:

N      :   45000 
NB     :    1024 
PMAP   : Column-major process mapping
P      :       2 
Q      :       2 
PFACT  :    Left 
NBMIN  :       2 
NDIV   :       2 
RFACT  :    Left 
BCAST  :  2ringM 
DEPTH  :       1 
SWAP   : Spread-roll (long)
L1     : no-transposed form
U      : transposed form
EQUIL  : no
ALIGN  : 8 double precision words

--------------------------------------------------------------------------------

- The matrix A is randomly generated for each test.
- The following scaled residual check will be computed:
      ||Ax-b||_oo / ( eps * ( || x ||_oo * || A ||_oo + || b ||_oo ) * N )
- The relative machine precision (eps) is taken to be               1.110223e-16
- Computational tests pass if scaled residuals are less than                16.0


HPL-NVIDIA ignores the following parameters from input file:
	* Broadcast parameters
	* Panel factorization parameters
	* Look-ahead value
	* L1 layout
	* U layout
	* Equilibration parameter
	* Memory alignment parameter

HPL-NVIDIA settings from environment variables:
--- DEVICE INFO ---
  Peak clock frequency: 1410 MHz
  SM version          : 80
  Number of SMs       : 108
-------------------
[HPL TRACE] cuda_nvshmem_init: max=8.9056 (2) min=8.9056 (3)
[WARNING] Change Input N 45000 to 44032
[HPL TRACE] ncclCommInitRank: max=0.8742 (1) min=0.8612 (2)
[HPL TRACE] cugetrfs_mp_init: max=0.9268 (0) min=0.9268 (3)
--- MEMORY INFO ---
DEVICE
  System           =      6.27460 GiB (MIN)      6.62617 GiB (MAX)      6.45039 GiB (AVG)
  HPL buffers      =      7.10353 GiB (MIN)      7.43948 GiB (MAX)      7.26955 GiB (AVG)
  Used             =     13.37814 GiB (MIN)     13.89377 GiB (MAX)     13.71994 GiB (AVG)
  Total            =     40.00000 GiB (MIN)     40.00000 GiB (MAX)     40.00000 GiB (AVG)
HOST
  HPL buffers      =      0.00021 GiB (MIN)      0.00021 GiB (MAX)      0.00021 GiB (AVG)
-------------------

 ... Testing HPL components ... 

 **** Factorization, m = 22528, policy = 0 **** 
avg time =    26.87 ms, avg =   879.13. min =   871.38 [rank 2, host node1, gpuID 0000:81:00.0], max =   887.01 GFLOPS

 **** Factorization, m = 22528, policy = 1 **** 
avg time =    37.69 ms, avg =   626.72. min =   514.56 [rank 2, host node1, gpuID 0000:81:00.0], max =   801.39 GFLOPS

 **** Factorization, m = 11264, policy = 0 **** 
avg time =    20.56 ms, avg =   574.59. min =   572.06 [rank 0, host node1, gpuID 0000:01:00.0], max =   577.14 GFLOPS

 **** Factorization, m = 11264, policy = 1 **** 
avg time =    34.58 ms, avg =   341.54. min =   276.17 [rank 2, host node1, gpuID 0000:81:00.0], max =   447.48 GFLOPS

 **** Factorization, m = 5120, policy = 0 **** 
avg time =    19.12 ms, avg =   280.72. min =   280.10 [rank 0, host node1, gpuID 0000:01:00.0], max =   281.34 GFLOPS

 **** Factorization, m = 5120, policy = 1 **** 
avg time =    32.92 ms, avg =   163.08. min =   132.14 [rank 2, host node1, gpuID 0000:81:00.0], max =   212.94 GFLOPS

 **** Factorization, m = 1024, policy = 0 **** 
avg time =    17.23 ms, avg =    62.32. min =    62.06 [rank 0, host node1, gpuID 0000:01:00.0], max =    62.58 GFLOPS

 **** Factorization, m = 1024, policy = 1 **** 
avg time =    31.25 ms, avg =    34.36. min =    27.40 [rank 2, host node1, gpuID 0000:81:00.0], max =    46.03 GFLOPS

 **** ncclBcast( Row ) **** 
avg time =     0.00 ms, avg = 253763.32. min = 192038.89 [rank 3, host node1, gpuID 0000:C1:00.0], max = 305040.29 GBS

 **** ncclAllGather( Col ) **** 
avg time =     0.00 ms, avg = 251344.06. min = 235469.70 [rank 3, host node1, gpuID 0000:C1:00.0], max = 263172.02 GBS

 **** Latency ncclAllGather, m = 1 **** 
avg time =     0.30 ms, avg =     0.06. min =     0.05 [rank 0, host node1, gpuID 0000:01:00.0], max =     0.06 GBS

 **** Latency ncclAllGather, m = 2 **** 
avg time =     0.29 ms, avg =     0.11. min =     0.11 [rank 2, host node1, gpuID 0000:81:00.0], max =     0.11 GBS

 **** Latency ncclAllGather, m = 32 **** 
avg time =     0.29 ms, avg =     1.79. min =     1.78 [rank 1, host node1, gpuID 0000:41:00.0], max =     1.79 GBS

 **** Latency ncclAllGather, m = 1024 **** 
avg time =     0.29 ms, avg =    57.24. min =    57.20 [rank 1, host node1, gpuID 0000:41:00.0], max =    57.32 GBS

 **** Latency ncclAllGather, m = 2048 **** 
avg time =     0.29 ms, avg =   114.35. min =   114.02 [rank 0, host node1, gpuID 0000:01:00.0], max =   114.80 GBS

 **** Latency Host MPI_Allgather, m = 1 **** 
avg time =     0.73 ms, avg =     0.02. min =     0.02 [rank 2, host node1, gpuID 0000:81:00.0], max =     0.02 GBS

 **** Latency Host MPI_Allgather, m = 2 **** 
avg time =     0.73 ms, avg =     0.04. min =     0.04 [rank 3, host node1, gpuID 0000:C1:00.0], max =     0.05 GBS

 **** Latency Host MPI_Allgather, m = 32 **** 
avg time =     0.84 ms, avg =     0.62. min =     0.61 [rank 2, host node1, gpuID 0000:81:00.0], max =     0.63 GBS

 **** Latency Host MPI_Allgather, m = 1024 **** 
avg time =     6.30 ms, avg =     2.67. min =     2.65 [rank 3, host node1, gpuID 0000:C1:00.0], max =     2.68 GBS

 **** Latency Host MPI_Allgather, m = 2048 **** 
avg time =    12.01 ms, avg =     2.79. min =     2.79 [rank 1, host node1, gpuID 0000:41:00.0], max =     2.80 GBS

 **** Latency ncclBcast, m = 1 **** 
avg time =     0.31 ms, avg =     0.03. min =     0.03 [rank 1, host node1, gpuID 0000:41:00.0], max =     0.03 GBS

 **** Latency ncclBcast, m = 32 **** 
avg time =     0.31 ms, avg =     0.84. min =     0.84 [rank 1, host node1, gpuID 0000:41:00.0], max =     0.84 GBS

 **** Latency ncclBcast, m = 1024 **** 
avg time =     0.31 ms, avg =    26.87. min =    26.83 [rank 1, host node1, gpuID 0000:41:00.0], max =    26.91 GBS

 **** Latency Host MPI_Bcast, m = 1 **** 
avg time =     0.12 ms, avg =     0.07. min =     0.06 [rank 3, host node1, gpuID 0000:C1:00.0], max =     0.07 GBS

 **** Latency Host MPI_Bcast, m = 32 **** 
avg time =     0.17 ms, avg =     1.58. min =     1.56 [rank 3, host node1, gpuID 0000:C1:00.0], max =     1.60 GBS

 **** Latency Host MPI_Bcast, m = 1024 **** 
avg time =     0.45 ms, avg =    18.61. min =    17.93 [rank 3, host node1, gpuID 0000:C1:00.0], max =    19.44 GBS

 **** GEMM **** 
avg time =    40.00 ms, avg = 18895.50. min = 18235.21 [rank 2, host node1, gpuID 0000:81:00.0], max = 19453.05 GFLOPS

 ... End of Testing HPL components ... 

[HPL TRACE] HPL_pdmatgen_gpu: max=0.0032 (1) min=0.0030 (2)
